\section{Cryptanalysis}
\label{sec:cryptan}

\outlinecomment{Get Aono, Moriai or others to contribute here.}

The general idea of trying to decode messages that are encrypted is
known as \emph{cryptanalysis}.  Here, we can deal only in public information;
we don't know how much more advanced intelligence agencies are.  We do
know that e.g. the RSA public key cryptosystem and the technique of
differential analysis were known to intelligence agencies for years,
perhaps decades, before they became public. \comment{cite}

\url{https://en.wikipedia.org/wiki/Advanced_Encryption_Standard#Known_attacks}
says, "For cryptographers, a cryptographic "break" is anything faster
than a brute-force attack â€“ i.e., performing one trial decryption for
each possible key in sequence (see Cryptanalysis). A break can thus
include results that are infeasible with current technology."
The page then goes on to list quite a number of attacks on AES, none
considered practical yet.

Given how long people have been working on cryptography, naturally
there are many techniques for attacking the ciphers.  We will address
just a couple of the modern techniques are known for what we might
call ``honest'' attacks on block cipher cryptography, granting the
assumption that the algorithm has no flaws and ignoring side channel
attacks (such as measuring the power consumed during encryption) and
attacks on infrastructure, people, etc.  (Some of these other attacks
are known as "black bag", or burglary, attacks, and some of which are
"rubber hose" attacks, extracting information from humans by coercion
or torture.  David Kahn referred to these latter attacks as
"practical cryptanalysis" in his classic book, \emph{The
  Codebreakers}.)

The older one of the two is differential cryptanalysis, apparently
discovered indepedently at least three times, publicly by Biham and
Shamir in the late 1980s, in the mid-70s by IBM (where the goal of
defending against the technique drove DES design decisions), and
earlier by NSA.
\url{https://en.wikipedia.org/wiki/Differential_cryptanalysis}
The second is linear crypanalysis, discovered by Mitsuru Matsui in
1990~\cite{matsui1993linear}.
\url{https://en.wikipedia.org/wiki/Linear_cryptanalysis}

\aono{}
(Other techniques that I know nothing about include integral,
algebraic, and biclique.)

Before talking about those two in any detail, there's a more
straightforward technique that leaks information about the plaintext,
if not the key itself: the birthday paradox.  But before we get into
the cryptanalysis itself, let's look a little bit at how the designers
built defenses into the block ciphers.

\subsection{Playing Defense Using Math}
\label{sec:defense}

\subsubsection{Entropy}

The single most important concept in cryptography -- indeed, in all of
information theory, and inarguably one of the most important in all of
computer science -- is Claude Shannon's \emph{entropy}.  The entropy is,
roughly, the amount of disorder in a sequence of data, or the amount
of information that must be transmitted to reconstruct the data.  When
the entropy of a cleartext message is high, it is hard to predict the
next symbol (bit or byte or letter or floating point number); purely
random data has very high entropy.  When it is low, the cleartext has
a skewed probability distribution such that, e.g., the letter 'e' or
the number 0 is more common than other values.

Of course, because encryption tries to hide information, encrypted
data looks as random as possible: it has very high entropy.  Thus, an
easy way to automate a test to see if we have successfully decrypted a
message is to calculate the entropy of our provisionally decrypted
plaintext; low entropy indicates a high probability that we have (at
least partially) succeeded.

The entropy is defined as
\begin{align}
H_{2}(X)=-\sum_{i=1}^{N} \frac{\operatorname{count}_{i}}{N} \log
_{2}\left(\frac{\operatorname{count}_{i}}{N}\right)
\end{align}
where $\operatorname{count}_i$ is the number of times that the $i$th
value appeared in our data, and $N$ is the number of different values
that show up in our data sequence.

Note that this definition of entropy does not take into account any
long-range structure; it is purely based on the overall probability of
the appearance of a given set of symbols.  Thus, to this function, the
strings 0101...01 and 000...01...111 have a bit-level entropy just as
high as a string composed of exactly 50/50 randomly chosen 0s and 1s,
although by applying a little intelligence we can easily reconstruct
the former but not the latter (at least, not exactly).  Many lossless
data compression programs operate on stateful \emph{sequences} of data
to predict the next symbol.  For example, the common Unix utilities
gzip and compress use the Lempel-Ziv
algorithm~\cite{compression:Ziv78}, which builds a dictionary of
sequences as it goes, so that repetitive data and long runs are
compressed effectively; this is especially good at sequences such as
0101...01 or 000...01...111.

(Taking still further advantage of brainpower, the ultimate in
compression is measured using the \emph{Kolmogorov complexity}: the
length of the shortest program that will reproduce the sequence.  This
has very low complexity for anything highly regular, as well as
seemingly irregular sequences such as the output of a pseudo-random
number generator or the digits of $\pi$.  I am not aware of any
automated form for calculating the Kolmogorov complexity of an
arbitrary sequence of data; in fact, it is known to be equivalent to
the halting problem~\cite{chaitin1975theory,chaitin1995program}.)

Many other tests for randomness are important and are used to validate
experimentally how completely an encryption system hides its data.  If
any pattern at all shows up in the ciphertext, that can be leveraged
in an attack, and is an indication that the encryption scheme is
poor.

This, obviously, is one of the motivations for the substitution phase
in a substitution-permutation network; with permutation only the
entropy of the original cleartext is preserved in the ciphertext, and
in some cases reveals a great deal about the original data.

\subsubsection{Diffusion, Confusion and the Avalanche Effect}
\label{sec:diffusion}

Claude Shannon, in his seminal article on the mathematics of
cryptography~\cite{shannon45}, defined two concepts he called
\emph{diffusion} and \emph{confusion}~\footnote{\url{https://en.wikipedia.org/wiki/Confusion_and_diffusion}}.  In diffusion, information from
the original plaintext message is spread across more than one symbol
in the output ciphertext; the farther the information spreads, the
better.  Shannon defined \emph{confusion} as making "the relation
between...[the ciphertext] $E$ and the...[key] $K$ a very complex and
involved one."

Feistel, who designed DES, called diffusion the \emph{avalanche
  effect}.  Webster and Tavares~\cite{webster86:avalanche} defined the
\emph{strict avalanche criterion} (SAC), requiring that changing a
single input bit flips each of the output bits with 50\% probability.

\emph{The Handbook of Applied Cryptography}~\cite{menezes1996handbook}
says the following (p. 20 in my edition):

\begin{quote}
A substitution in a round is said to add \emph{confusion} to the
encryption process whereas a transposition [permutation] is said to
add \emph{diffusion}. Confusion is intended to make the relationship
between the key and the ciphertext as complex as possible.  Diffusion
refers to rearranging or spreading out the bits in the message so that
any redundancy in the plaintext is spread out over the ciphertext.
\end{quote}

\aono{} In DES, confusion is achieved by the nonlinearity of the
S-boxes.  Diffusion is achieved by the shuffling of the bits between
S-box operations, coupled with the fact that the S-boxes use multiple
bits in an operation.

These two concepts ultimately underly much of the process of
cryptanalysis: spreading data broadly reduces the relationship
exhibited by two ciphertexts even when the plaintexts are closely
related, expanding the search space; and the nonlinearity means that
even a large set of straightforward equations is not enough to simply
and mathematically relate the input and output.

\subsection{The Birthday Paradox}
\label{sec:birthday}

The basic idea of the birthday paradox is a familiar problem in
elementary probability:
With $n$ people in a room, what is the probability of two of
them sharing a birthday?  More concretely, how many people have to be
in the room before the probability of two sharing a birthday exceeds
50\%?  It's a surprisingly small number, but it's not a true paradox in
the logical sense.  This is also called the \emph{pigeonhole principle} or
the \emph{hash collision probability}.

Modern ciphers are designed so that the ciphertext (output of the
encryption) looks random; we can say that the ciphertext has very high
entropy.  If the block size is $n$ bits, each of the $2^n$ possible
bit combinations should be an equally likely result of encrypting a
data block.  (The encryption is deterministic, but the output looks
random.)

If we monitor a long stream of encrypted data, there is some
probability that we will find two blocks that have the same
ciphertext.  We call that a \emph{collision}.  If the two blocks were
encrypted with the same key, we gain information about the plaintext.

The occurence of collisions may seem counterintuitive: if two
different plaintext blocks have the same ciphertext, wouldn't that
mean that the encryption process is lossy, making it impossible to
reliably decrypt that ciphertext back to both original blocks?  As
will will see in Sec.~\ref{sec:limitations} \comment{and mentioned earlier}, more information is
involved in both the encryption and decryption than just that specific
block and the key when using \emph{cipher block chaining} (CBC mode).

In CBC mode, the information gained on the occurence of a collusion is
the XOR of two plaintext blocks.  If block numbers $i$ and $j$ have
the same ciphertext $c[i] = c[j]$, then the ciphertext is the XOR of
the plaintext of the predecessor blocks, $p[i-1]$ and $p[j-1]$.  If
the attacker could choose which two blocks, this could be incredibly
valuable information; given that it's just two random blocks, it's
less so, but not unimportant.  \comment{notation probably needs to be
  explained, maybe back up in Intro, or where block ciphers are
  explained.}

If the block size is $n$ bits, there are $N = 2^n$ possible
ciphertexts, and if the number of blocks encrypted is the square root
of that number, $2^{n/2}$, the probability of at least one collision
is above 39\%, which arises in the limit as the expression
$1-1/\sqrt{e}$.  (See the appendix of these notes for some example
calculations.)  Sec. 2.2 of Bhargavan and Leurent has a compact
description of the commonly-quoted square root limit, as well as
a description of why and how much to be conservative relative to that
value~\cite{bhargavan2016practical}~\footnote{\url{https://sweet32.info/}.}.

A 39\% chance of disclosing some information will generally be
considered to be an unacceptably large probability, leaving us with
the question of when to rekey.  Let us assume, for example, that we
want the probability of an attacker recovering a plaintext block using
the birthday attack to be less than (for example) one in a billion.

Given $D = 2^d$ ciphertext blocks, then the expected number of
collisions is approximately $2^{2d-n-1}$.  For our one-in-a-billion
probability, using $\log_2(10^9) \approx 30$, we need to set up our
session lifetime such that $2d-n-1 < -30$, or $d < (n-29)/2$.

Since DES has only a 64-bit block, we should set $d \le 17$.  That's a
startlingly small number: $D = 2^{17}$ blocks of eight bytes each
would requires us should limit the use of a single key to one
megabyte, an impractically small size.  If we are trying to protect a
40Gbps link -- a common backbone bandwidth today, and coming to the
home in the foreseeable future -- the key would have to be changed
once every 200$\mu$sec.

If, on the other hand, we relax the disclosure probability to one in a
million, we can raise our limit by a factor of a thousand and change
keys once every gigabyte.  On our hypothetical 40Gbps link, that still
means changing keys once every 200msec.

AES uses $n = 128$, a block size twice as large.  Now we only need $d
\le 49$ to achieve the one-in-a-billion probability.  $D = 2^{49}$
times our block size of 16 bytes equals 8 terabytes, about 1,600
seconds on our 40Gbps link, giving a recommended cryptoperiod of just
under half an hour. \comment{Tie this to intro, IPsec and TLS.}

A few points are worth emphasizing:
\begin{enumerate}
\item what's disclosed here is not the key but is plaintext-related,
  but not even pure plaintext; however, cryptanalysts can do amazing
  things with small amounts of data;
\item this depends only on the block size of a block cipher, not on the
   key length;
\item part of this arises due to the CBC (cipher block chain) mode,
  but ECB (electronic code book) mode is worse; and
\item given that there are still other attacks, minimizing the amount of
   data under any given encryption key is still good practice.
\end{enumerate}

Next, we look at some cryptanalysis techniques.

\subsection{Differential Cryptanalysis}

Biham and Shamir wrote a seminal paper~\cite{biham1991differential},
then an easy-to-read book~\cite{biham1993differential-book} on their
rediscovery of differential cryptanalysis.  The goal of DC is to
recover the key used for a session, so it is potentially far more
serious than the birthday attack.

The book says, ``An interesting feature of the new attack is that it
can be applied with the same complexity and success probability even
if the key is frequently changed and thus the collected ciphertexts
are derived from many different keys.''  \aono{} That's pretty alarming.  This
appears in a paragraph discussing chosen plaintext, so it may be
restricted to that case.  I infer from this that rather than needing
the cumulative accretion of information, each trial is independent.

\comment{rework this par}
The attack works with either \emph{chosen} plaintext (in which the attacker
says, "Please encrypt this message for me, and give me the
ciphertext,") or \emph{known} plaintext (in which the attacker knows that
the message is a repetition of "HEILHILTER", as figured prominently in
the cracking ofo the Enigma machine in WWII; modern HTTPS connections and
SMTP (email) connections have enough predictability in their content
to provide a similar fulcrum for such a lever; see Sec. 2.x of these
notes).  There is a substantial difference in the complexity of the
two attacks (see Tab. 2.1 in the book).  Known plaintext takes *waaay*
more trials to succeed.  \comment{informal}


The key idea is the construction of \emph{differentials} (hence the name)
from specific S boxes.  Take two possible inputs to an S box, $X_1$
and $X_2$.  We can assume the subkey has been XORed into both $X_1$
and $X_2$ already.  $Y_1$ and $Y_2$ are the corresponding outputs of the
S box.  We know that if $X_1 = X_2$, then $Y_1 = Y_2$ and also $X_1 +
X_2 = 0$ (again, here '+' is addition modulo two, or XOR).

For the total encryption algorithm, changing one bit in the input
should result in about half the output bits changing.  The S box
should be similar; small changes in the input should mean large
changes in the output.  In fact, the S box is small enough that we can
exhaustively analyze its inputs.  We also know some rules that were
chosen during the design phase, for example, changing \emph{one} bit
in the six-bit input should result in changes to at least \emph{two}
bits in the four-bit output.

A differential table for each S box is constructed by listing all
$2^6$ possible XORs $X_1 + X_2$, and collecting the stats on $Y_1 +
Y_2$.  From the differences found here, we can work backwards to find
with ``high'' probability (slightly higher than completely random)
some characteristics of the outputs of the \emph{previous}
round of the encryption.

The overall attack is performed by encrypting a bunch of
randomly-chosen \emph{pairs} of plaintexts (chosen as a pair; first a
completely random one, then a second by XORing in a specific value)
and comparing their ciphertexts until we find an output pair of
ciphertexts that fit comfortably with the difference tables we have
pre-computed for the S boxes.  Repeat until we have built up some
statistics about the right set of bits in the output, and from that we
can take a probabilistic guess at the subkey in the last round.  Based
on that guess, we can narrow down the set of subkeys for the
next-to-last round, rinse and repeat.  It's still a computationally
intensive, tedious process, but much less than brute force.  Roughly,
the probability of getting the ciphertext pairs we need is
proportional to $1/p_D$, \comment{? should be a small number, not a large one -- inverted?} where $p_D$ is the differential probability
in the table we are looking for, which may be quite small.  (This
seems to me that keeping a history of things you've already tried
would increase the probability of finding a pair you like, so I'm
still puzzled by the assertion above that this works even if the key
is being changed frequently.) \aono{}

Ultimately, this attack is considered practical against ordinary DES.
Biham and Shamir estimated (p. 8, p. 61) that DES and DES-like
systems, even with the full sixteen rounds, could be broken using
$2^{43}$ to $2^{47}$ chosen plaintext/ciphertext pairs.  With 8-byte
blocks, that's encryption of 64 terabytes up to a petabyte.  If the
system under attack can encrypt a 40Gbps link at line rate, that's
only a few hours up to a couple of days of data.

Triple-DES would be much, much longer, but 3-DES is still considered
obsolete due to the birthday paradox attack above.  AES is stronger
against DC, so this attack is of less help against
properly-implemented, modern systems.

\aono{}

\subsection{Linear Cryptanalysis}

Linear cryptanalysis (LC) is a known plaintext attack, developed by
Mitsuru Matsui in 1990 after being inspired by differential
cryptanalysis~\cite{matsui1993linear,heys:tutorial}.  The key idea is
to build a linear approximation of an S box, allowing the attacker to
calculate possible keys in reverse more quickly.

If a cipher is good, every individual bit of the output should have a
50\% probability of being 0 and a 50\% probability of being 1.
Likewise, there should be no obvious relationship between the input
and output bits (e.g., "If the third input bit is 1, the seventh
output bit is 0.")  However, it is possible that some \emph{combinations}
of bits don't appear with equal probability. For example, the bit
string composed of the first and third input bits and the second and
fourth output bits should have all sixteen combinations 0000, 0001,
..., 1111 with equal probability, but perhaps there is a bias.  If $P_i$
is the $i$th input plaintext bit and $C_i$ is the $i$th output ciphertext
bit, we can construct an equation like

$L = P_1 + P_3 + C_2 + C_4$

(where '+' is modulo 2 addition, or XOR, here).  We say that such a
combination has a bias if the probability of $L$ being 0 is noticeably
different from 50\%.

Such a combination is a \emph{linear} combination of bits.  It is known (\aono{by whom?}) that if the S-boxes in our cipher are fully linear, then the
cipher can be easily broken.  Therefore, designers always use
nonlinear S-boxes, but some bias such as this may be discoverable.

The basic idea of LC, then, is to find such sets of bits that exhibit
some bias and use some algebra to recover a few bits of the subkey
used in the last round of the cipher, rinse and repeat.  The trick is
to find the right relationships showing a detectable bias, as was done
with differential analysis.  This is done by examining the math of the
S-boxes in detail; as far as I can tell, this phase of the operation
is done by very smart humans. \aono{}

If the attacker can find a combination with a bias of $\epsilon$, then
$1/\epsilon^2$ plaintext-ciphertext pairs are required to find some bits
of the subkey.

This is done by taking multiple expressions like the above, combining
them using Matsui's ``Piling Up Principle'' where you track the biases
multiplied together to make a linear approximation of the nonlinear
S-box.

With this linear approximation, it is possible to find correlations
between the output of the \emph{next-to-last} round of the cipher and the
original input qubits, from which it is possible to recover
information about the \emph{subkey} used in the last round of the cipher.

Ultimately, this doesn't give you complete information about the key
or the plaintext, but substantially reduces the work needed to find
the full key by guiding a search, rather than just testing keys at
random.

\aono{} Linear cryptanalysis is considered to be a very general
technique, but there do not appear to have been extensive attempts to
apply it to AES.  Indeed, AES (which was developed in the 1990s from
the proposed cipher known as Rijndael) was developed specifically with
the idea of not being vulnerable to either DC or LC.

I found Heys' tutorial to be clear and helpful in understanding LC.

\subsection{Known and chosen plaintexts in real systems}

\outlinecomment{Parts of this will make more sense after getting
  through the below.  Maybe this section should be moved below the
  next section.  Also, some pictures will definitely help here.}

\comment{This is fairly ad hoc, a set of rough notes I wrote up
  explaining some ideas without enough rigor.  Would love to have
  someone with more sense than me look this section over.}

Modern HTTPS (web) and SMTP (email) connections have a lot of
predictability in their content, with commands like 'HELO' and
'HTTP/1.1' being standard parts of an exchange.  In
Sec.~\ref{sec:tls}, we'll see more detail about how this encryption is
done using a protocol called Transport Layer Security, or TLS, but for
the moment we'll only focus on the message contents and the fact that
they are pretty predictable.  Thus, it's reasonable to consider
attacks on TLS to be known-plaintext attacks, and in fact there are
cases where we can create chosen-plaintext attacks.

\aono{can you come up with a better example of how chosen-plaintext works?}
Consider, for example, the connection between your laptop and your
email server, whether Gmail or your organization's server.  Assume
that I, an attacker, can send you email and can observe your encrypted
connection to your server (perhaps I control a router somewhere
between your server and your machine).  I can send you an email
message that contains, for example, the strings 0x000000000000000 (15
zero bytes in a row) and 0x000000010000000 (with a one in the middle).
If your cipher block size is 8 bytes, as in DES, I know that one of
the encrypted blocks will be all zeroes and one will have exactly one
bit set, even if I have trouble controlling the exact position within
the overall stream.  I capture the ciphertext blocks, and compare
them.  This single-bit difference between two blocks helps me with the
attack.  All I have to do to execute a basic chosen-plaintext attack
is to send you email and watch the resulting packets flow between your
machines!

The success of such an attack requires a lot of assumptions about
which parts of the entire process I can observe and which I can
control, but using the principle of being conservative on security,
assuming an attacker can force the choice of plaintext passed between
two nodes through an encrypted connection is not unreasonable in
today's richly interwoven distributed systems.

IPsec (Sec.~\ref{sec:ipsec}) presents another potential vulnerability:
one encrypted connection between two gateways (known as a
\emph{Security Association}, which we will see below) may carry data
encrypted for many machines, providing a broad attack surface for
attempts to execute chosen plaintext attacks.  If an attacker manages
to install a program on only one laptop (e.g., via email, or while the
user is on an unprotected network such as at a coffeeshop), they can
cause a system to send out arbitrarily chosen packets that will cross
the tunnel, controlling the content if not always the sequence of
packets, which may be interleaved with traffic from other applications
or end systems.  Because IPsec encrypts the whole packet, the attacker
may not be able to tell immediately which packets came from the
compromised laptop and which from an unaffected laptop.

For every IP packet from a laptop to e.g. an email server passing
through the IPsec tunnel, the IP header portion is going to be exactly
the same, and its position in the encrypted stream is very easy to
identify.  This led to some of the decisions around the use of CBC, I
believe; I'm not aware of any deeper features intended to further
obscure the location of such predictable data.  \ddp{}

In short, defenders should work on the assumption that a noticeable
fraction of the plaintext carried in a tunnel is known even under
benign circumstances, and that it is not hard for an attacker
to mount a chosen plaintext attack.

\subsection{Notes \& References}

Some of the references I used for this section:

The best source on the current state of the birthday paradox is Bhargavan and Leurent~\cite{bhargavan2016practical}.  Other references include Miessler~\footnote{\url{https://danielmiessler.com/study/birthday_attack/}} and Wolfram Mathworld~\footnote{\url{http://mathworld.wolfram.com/BirthdayAttack.html}}.

1.

2. Abdalla and Bellare,
\url{https://link.springer.com/chapter/10.1007/3-540-44448-3_42}
\url{http://cseweb.ucsd.edu/~mihir/papers/rekey.html}
found via
\url{http://cseweb.ucsd.edu/~mihir/}

That paper talks about differential/linear cryptanalysis and about the
birthday paradox, saying block size $k$ needs to be rekeyed every
$2^{k/2}$ blocks.

Bellare et al, A concrete security treatment of symmetric encryption:
analysis of the DES modes of operation
abstract from STOC 1997
https://ieeexplore.ieee.org/abstract/document/646128
full paper at
\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.4734&rep=rep1&type=pdf}
Focuses on DES, which was in the process of being superseded by AES
even in 1997, but the content of the paper is valuable with respect to
CBC.  I found the paper a tough read when trying to figure out how to
apply the equations.

